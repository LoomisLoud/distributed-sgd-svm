{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from svm_function import contains_CCAT, calculate_loss, is_support, gradient_update\n",
    "import data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector initialization (using dict formats)\n",
    "Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "iterator = data.get_batch(batch_size)\n",
    "\n",
    "# Pour avoir le prochain batch of size \"batch_size\"\n",
    "batch = next(iterator)\n",
    "\n",
    "# Pour avoir un sample\n",
    "one_batch = dict(list(batch.values())[51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "dict2list = lambda dic: [(k, v) for (k, v) in dic.items()]\n",
    "list2dict = lambda lis: dict(lis)\n",
    "\n",
    "labels = data.load_labels()\n",
    "final_labels = list2dict(map(contains_CCAT,dict2list(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization for weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 47236\n",
    "w = np.random.uniform(low=-1, high = 1, size = num_features)\n",
    "dict_w={}\n",
    "for (i,j) in enumerate(w):\n",
    "    dict_w[i+1]=j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing functions\n",
    "Loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996.4571557372293"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(final_labels, batch, dict_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_support(-1, one_batch, dict_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{440: 0.0415939463627629,\n",
       " 798: 0.035174063491958,\n",
       " 1055: 0.0578983469257403,\n",
       " 1156: 0.0575883090463427,\n",
       " 1523: 0.0363421231109103,\n",
       " 1553: 0.0542308866146216,\n",
       " 1681: 0.0323842454160338,\n",
       " 2181: 0.0735909947470534,\n",
       " 2218: 0.146079519099118,\n",
       " 2408: 0.0608667014417416,\n",
       " 2650: 0.0629858536518628,\n",
       " 2818: 0.047694724448736,\n",
       " 3240: 0.0308303146028078,\n",
       " 3406: 0.046079149342318,\n",
       " 3583: 0.0388911301590628,\n",
       " 3746: 0.0924687005044042,\n",
       " 4271: 0.0562549844023875,\n",
       " 4922: 0.0807135265537715,\n",
       " 5041: 0.116365242084534,\n",
       " 5186: 0.0597545889729037,\n",
       " 5254: 0.0694635320871477,\n",
       " 5261: 0.0426442473384211,\n",
       " 5436: 0.15792685431611,\n",
       " 5483: 0.0763321226491045,\n",
       " 5631: 0.151728329600831,\n",
       " 5992: 0.0602793027950793,\n",
       " 6164: 0.0317457151300041,\n",
       " 6478: 0.0505415436693516,\n",
       " 6694: 0.0682672306475561,\n",
       " 6932: 0.0665137368995717,\n",
       " 7119: 0.0227036375433203,\n",
       " 8123: 0.0547806590777755,\n",
       " 8145: 0.0240303955091146,\n",
       " 8274: 0.0529421117756231,\n",
       " 8609: 0.0192320608569829,\n",
       " 8647: 0.0657714693594091,\n",
       " 8768: 0.0605699962136763,\n",
       " 8897: 0.0571189628002249,\n",
       " 8901: 0.0424057163949551,\n",
       " 9265: 0.0516523613486832,\n",
       " 9273: 0.0597545889729037,\n",
       " 9303: 0.0556469567318795,\n",
       " 9380: 0.0652971350593148,\n",
       " 9557: 0.0518507162605685,\n",
       " 10182: 0.0230247688574822,\n",
       " 10242: 0.027079906559647,\n",
       " 10468: 0.117652891803723,\n",
       " 10521: 0.0721966815542716,\n",
       " 10953: 0.0712022183262943,\n",
       " 11813: 0.0743378632896531,\n",
       " 12144: 0.106231086920015,\n",
       " 12219: 0.0286231929139016,\n",
       " 12857: 0.0505833116923779,\n",
       " 13627: 0.0343684625064434,\n",
       " 13658: 0.0660799388528609,\n",
       " 14311: 0.054558215772727,\n",
       " 14331: 0.0966382064152666,\n",
       " 14384: 0.0272351907925279,\n",
       " 14476: 0.030089644046816,\n",
       " 14615: 0.0561312999659152,\n",
       " 14780: 0.0807541882279609,\n",
       " 14844: 0.0317176107022582,\n",
       " 15152: 0.0237505344477614,\n",
       " 15798: 0.0312263489332598,\n",
       " 15846: 0.106420447536867,\n",
       " 15953: 0.0941183468722989,\n",
       " 16219: 0.0511159380285937,\n",
       " 16985: 0.0560086726743052,\n",
       " 17800: 0.0618412757777464,\n",
       " 17866: 0.0738004481863196,\n",
       " 18150: 0.0455105914063301,\n",
       " 18273: 0.0883789087305239,\n",
       " 19308: 0.0516015855123408,\n",
       " 19333: 0.128740871712331,\n",
       " 19436: 0.108206493688269,\n",
       " 19493: 0.084840989213677,\n",
       " 19586: 0.0924768665799461,\n",
       " 19892: 0.064613800876924,\n",
       " 20035: 0.070854387817397,\n",
       " 20036: 0.0654731778296364,\n",
       " 20104: 0.0628869063232902,\n",
       " 20122: 0.0511376599188545,\n",
       " 20213: 0.0658318791571902,\n",
       " 20215: 0.217799009262113,\n",
       " 21186: 0.0270069333862331,\n",
       " 21989: 0.144283810006778,\n",
       " 22074: 0.0444488231775938,\n",
       " 22929: 0.121935568167121,\n",
       " 23323: 0.0311721876542306,\n",
       " 23373: 0.0483257323052847,\n",
       " 23446: 0.0428379691780883,\n",
       " 23578: 0.0421478330411495,\n",
       " 23803: 0.0542506569340679,\n",
       " 23903: 0.0777879360210742,\n",
       " 24081: 0.0499692105739626,\n",
       " 24479: 0.0437865213542365,\n",
       " 25067: 0.0545244838307036,\n",
       " 25081: 0.0270231176020233,\n",
       " 25226: 0.0326934409298757,\n",
       " 25503: 0.0373968188010972,\n",
       " 26304: 0.0387777071364796,\n",
       " 26660: 0.130191962651221,\n",
       " 26667: 0.0805437339712098,\n",
       " 26781: 0.0760803470820964,\n",
       " 27307: 0.0282832782498568,\n",
       " 27385: 0.0316329418632452,\n",
       " 28079: 0.0928699680405875,\n",
       " 28343: 0.0445897984938103,\n",
       " 28628: 0.116201491827089,\n",
       " 29244: 0.0901830082892737,\n",
       " 29257: 0.0492653868894149,\n",
       " 29312: 0.118576889023683,\n",
       " 29685: 0.0628869063232902,\n",
       " 29719: 0.0385679874615686,\n",
       " 29755: 0.0969974307004931,\n",
       " 29857: 0.161014275148091,\n",
       " 30241: 0.0457618184272375,\n",
       " 30631: 0.0896684592778109,\n",
       " 31829: 0.0994258584066446,\n",
       " 32048: 0.0659534672219721,\n",
       " 32244: 0.0431501741733425,\n",
       " 32674: 0.0352528753573215,\n",
       " 32985: 0.0463157673771784,\n",
       " 33011: 0.0337211335091768,\n",
       " 33035: 0.0638769067179761,\n",
       " 33170: 0.0253764957491186,\n",
       " 33547: 0.0638872509838144,\n",
       " 33588: 0.269717184503149,\n",
       " 33730: 0.0363188886025506,\n",
       " 33768: 0.0476776341479939,\n",
       " 34057: 0.0857870510464114,\n",
       " 34328: 0.0562239631886233,\n",
       " 34367: 0.0486858077874788,\n",
       " 34428: 0.0575584037686214,\n",
       " 34824: 0.0604446914366098,\n",
       " 34910: 0.0214397883026569,\n",
       " 35110: 0.0750105164324464,\n",
       " 35122: 0.0391028461412679,\n",
       " 35381: 0.0294859228242776,\n",
       " 35388: 0.0517596219496176,\n",
       " 35945: 0.0376646812499263,\n",
       " 36281: 0.0447536913049663,\n",
       " 36638: 0.0436401235430953,\n",
       " 36643: 0.0691549759847488,\n",
       " 37056: 0.0988702334147719,\n",
       " 37141: 0.0470428418823079,\n",
       " 37267: 0.0331142847462927,\n",
       " 37461: 0.0327657500578682,\n",
       " 37481: 0.0297870621657864,\n",
       " 38159: 0.0349191194650348,\n",
       " 38372: 0.0500495878473579,\n",
       " 39164: 0.0493227379548948,\n",
       " 39513: 0.053400227146337,\n",
       " 39515: 0.106420447536867,\n",
       " 39525: 0.0332925075900735,\n",
       " 39556: 0.084605227998111,\n",
       " 39724: 0.0271941632959496,\n",
       " 39913: 0.0561303179769221,\n",
       " 40238: 0.077204729482776,\n",
       " 40332: 0.0648307521113099,\n",
       " 40336: 0.0344767782783795,\n",
       " 40532: 0.0964884559481873,\n",
       " 41064: 0.0440586708034532,\n",
       " 41444: 0.141370313703887,\n",
       " 41515: 0.0679831162985581,\n",
       " 41743: 0.0460029552929458,\n",
       " 41914: 0.0426383577921544,\n",
       " 42157: 0.0632362492878634,\n",
       " 42199: 0.0395610824808778,\n",
       " 42321: 0.0496129833037302,\n",
       " 42362: 0.0400512678217293,\n",
       " 42679: 0.0529174812017172,\n",
       " 42706: 0.0628376873872792,\n",
       " 42936: 0.0434990223937125,\n",
       " 42999: 0.0899908842123933,\n",
       " 43102: 0.157903638096313,\n",
       " 43325: 0.0604863382170184,\n",
       " 43636: 0.0636359450194209,\n",
       " 44230: 0.0660146498553147,\n",
       " 44599: 0.049730745708593,\n",
       " 44652: 0.0738004481863196,\n",
       " 44749: 0.0586856784326325,\n",
       " 44996: 0.0670967776289265,\n",
       " 45332: 0.0826191014202595,\n",
       " 45357: 0.125984312077971,\n",
       " 46180: 0.0740727176010463,\n",
       " 46195: 0.0536495166050098,\n",
       " 46545: 0.0119393563001163,\n",
       " 46694: 0.0333137292198791}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_update(-1, one_batch, dict_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With numpy (before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hinge loss function, but we could also use more standards loss later \n",
    "def hinge_loss(y, X, w):\n",
    "    return np.clip(1 - y * (X @ w), 0, np.inf)\n",
    "\n",
    "def calculate_primal_objective(y, X, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    X: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    v = hinge_loss(y, X, w)\n",
    "    return np.sum(v) + lambda_ / 2 * np.sum(w ** 2)\n",
    "    \n",
    "def accuracy(y1, y2):\n",
    "    return np.mean(y1 == y2)\n",
    "\n",
    "def prediction(X, w):\n",
    "    return (X @ w > 0) * 2 - 1\n",
    "\n",
    "def calculate_accuracy(y, X, w):\n",
    "    predicted_y = prediction(X, w)\n",
    "    return accuracy(predicted_y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, cost=8379144195.244217\n",
      "iteration=10000, cost=2569445.6572731775\n",
      "iteration=20000, cost=1606766.2442051857\n",
      "iteration=30000, cost=1186094.9340004118\n",
      "iteration=40000, cost=1007087.3735559117\n",
      "iteration=50000, cost=1131750.4592788236\n",
      "iteration=60000, cost=746568.3302071713\n",
      "iteration=70000, cost=686601.2487545054\n",
      "iteration=80000, cost=639245.6090707903\n",
      "iteration=90000, cost=609544.1412513953\n",
      "iteration=100000, cost=592931.1118818587\n",
      "iteration=110000, cost=573526.3415829436\n",
      "iteration=120000, cost=544963.1939392536\n",
      "iteration=130000, cost=561023.8797291429\n",
      "iteration=140000, cost=540350.6845321481\n",
      "iteration=150000, cost=507792.3903086364\n",
      "iteration=160000, cost=502465.41475056723\n",
      "iteration=170000, cost=517028.95424482174\n",
      "iteration=180000, cost=493117.70732193266\n",
      "iteration=190000, cost=480501.13065999973\n",
      "iteration=200000, cost=474716.177453672\n",
      "iteration=210000, cost=463009.580368568\n",
      "iteration=220000, cost=489125.67552255426\n",
      "iteration=230000, cost=453521.6787539215\n",
      "iteration=240000, cost=448108.2869431452\n",
      "iteration=250000, cost=445906.1165676823\n",
      "iteration=260000, cost=461247.54894997936\n",
      "iteration=270000, cost=437701.7555387473\n",
      "iteration=280000, cost=432550.7712219345\n",
      "iteration=290000, cost=435167.4504250936\n",
      "iteration=300000, cost=431065.9299582231\n",
      "iteration=310000, cost=424295.40016185323\n",
      "iteration=320000, cost=440874.85483057675\n",
      "iteration=330000, cost=417277.3890537086\n",
      "iteration=340000, cost=411974.2565442774\n",
      "iteration=350000, cost=410730.59596816567\n",
      "iteration=360000, cost=411494.17042433517\n",
      "iteration=370000, cost=406922.2119615289\n",
      "iteration=380000, cost=408591.6949327063\n",
      "iteration=390000, cost=400107.8842351242\n",
      "iteration=400000, cost=414403.7927773239\n",
      "iteration=410000, cost=396503.9845491099\n",
      "iteration=420000, cost=394534.747889781\n",
      "iteration=430000, cost=392005.4235290594\n",
      "iteration=440000, cost=388648.74695334706\n",
      "iteration=450000, cost=388141.30795560416\n",
      "iteration=460000, cost=385195.28090662806\n",
      "iteration=470000, cost=388550.16011491226\n",
      "iteration=480000, cost=381609.3191789538\n",
      "iteration=490000, cost=379773.82065835\n",
      "training accuracy = 0.7218\n"
     ]
    }
   ],
   "source": [
    "def sgd_for_svm_demo(y, X):\n",
    "    \n",
    "    max_iter = 500000\n",
    "    gamma = 1\n",
    "    lambda_ = 0.00001\n",
    "    random.seed(2)\n",
    "    \n",
    "    num_examples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    # convert to list for main loop\n",
    "    w = list(w)\n",
    "    \n",
    "    def is_support(y_n, x_n, w):\n",
    "        \"\"\"a datapoint is support if max{} is not 0. \"\"\"\n",
    "        dot_prod = 0\n",
    "        for (x_n_i, w_i) in zip(x_n,w):\n",
    "            dot_prod += x_n_i*w_i\n",
    "        return y_n * dot_prod < 1\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        \n",
    "        # n = sample one data point uniformly at random data from x\n",
    "        n = random.randint(0,num_examples-1)\n",
    "        \n",
    "        x_n, y_n = X[n], y[n]\n",
    "        \n",
    "        # convert to list for main loop\n",
    "        x_n = list(x_n)\n",
    "        \n",
    "        ################ MAIN LOOP ##################\n",
    "        ############# GRADIENT UPDATE ###############\n",
    "        \n",
    "        if(is_support(y_n, x_n, w)):\n",
    "            grad = [- y_n * x_n_i + lambda_ * w_i for (x_n_i ,w_i) in zip(x_n,w)]\n",
    "            w = [w_i - gamma/(it+1) * grad_i for (w_i,grad_i) in zip(w,grad)]\n",
    "        \n",
    "        else:\n",
    "            grad = [lambda_ * w_i for w_i in w]\n",
    "            w = [w_i - gamma/(it+1) * grad_i for (w_i,grad_i) in zip(w,grad)]\n",
    "        ################ END OF LOOP ################\n",
    "        \n",
    "        if it % 10000 == 0:\n",
    "            cost = calculate_primal_objective(y, X, np.array(w), lambda_)\n",
    "            print(\"iteration={i}, cost={c}\".format(i=it, c=cost))\n",
    "    \n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, X, w)))\n",
    "\n",
    "sgd_for_svm_demo(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM\n",
    "#### Version du lab du cours de ML - inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_stochastic_gradient(y, X, w, lambda_, n):\n",
    "    \"\"\"compute the stochastic gradient of loss plus regularizer.\n",
    "    X: the dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    n: the index of the (one) datapoint we have sampled\n",
    "    \"\"\"\n",
    "    # Be careful about the constant N (size) term!\n",
    "    # The complete objective for SVM is a sum, not an average as in earlier SGD examples!\n",
    "    def is_support(y_n, x_n, w):\n",
    "        \"\"\"a datapoint is support if max{} is not 0. \"\"\"\n",
    "        return y_n * x_n @ w < 1\n",
    "    \n",
    "    x_n, y_n = X[n], y[n]\n",
    "    \n",
    "    grad = - y_n * x_n.T if is_support(y_n, x_n, w) else np.zeros_like(x_n.T)\n",
    "    grad = np.squeeze(grad) + lambda_ * w\n",
    "    \n",
    "    return grad"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
