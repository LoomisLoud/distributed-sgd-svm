{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will re-use the CERN dataset from project 1, available from https://inclass.kaggle.com/c/epfml-project-1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,) (5000, 30)\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_csv_data\n",
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "\n",
    "np.random.seed(1)\n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare cost and prediction functions\n",
    "### To check if the model runs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hinge loss function, but we could also use more standards loss later \n",
    "def hinge_loss(y, X, w):\n",
    "    return np.clip(1 - y * (X @ w), 0, np.inf)\n",
    "\n",
    "def calculate_primal_objective(y, X, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    X: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    v = hinge_loss(y, X, w)\n",
    "    return np.sum(v) + lambda_ / 2 * np.sum(w ** 2)\n",
    "def accuracy(y1, y2):\n",
    "    return np.mean(y1 == y2)\n",
    "\n",
    "def prediction(X, w):\n",
    "    return (X @ w > 0) * 2 - 1\n",
    "\n",
    "def calculate_accuracy(y, X, w):\n",
    "    predicted_y = prediction(X, w)\n",
    "    return accuracy(predicted_y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement stochastic gradient descent + op√©rations sur des listes dans la main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, cost=8379144195.244217\n",
      "iteration=10000, cost=2569445.6572731775\n",
      "iteration=20000, cost=1606766.2442051857\n",
      "iteration=30000, cost=1186094.9340004118\n",
      "iteration=40000, cost=1007087.3735559117\n",
      "iteration=50000, cost=1131750.4592788236\n",
      "iteration=60000, cost=746568.3302071713\n",
      "iteration=70000, cost=686601.2487545054\n",
      "iteration=80000, cost=639245.6090707903\n",
      "iteration=90000, cost=609544.1412513953\n",
      "iteration=100000, cost=592931.1118818587\n",
      "iteration=110000, cost=573526.3415829436\n",
      "iteration=120000, cost=544963.1939392536\n",
      "iteration=130000, cost=561023.8797291429\n",
      "iteration=140000, cost=540350.6845321481\n",
      "iteration=150000, cost=507792.3903086364\n",
      "iteration=160000, cost=502465.41475056723\n",
      "iteration=170000, cost=517028.95424482174\n",
      "iteration=180000, cost=493117.70732193266\n",
      "iteration=190000, cost=480501.13065999973\n",
      "iteration=200000, cost=474716.177453672\n",
      "iteration=210000, cost=463009.580368568\n",
      "iteration=220000, cost=489125.67552255426\n",
      "iteration=230000, cost=453521.6787539215\n",
      "iteration=240000, cost=448108.2869431452\n",
      "iteration=250000, cost=445906.1165676823\n",
      "iteration=260000, cost=461247.54894997936\n",
      "iteration=270000, cost=437701.7555387473\n",
      "iteration=280000, cost=432550.7712219345\n",
      "iteration=290000, cost=435167.4504250936\n",
      "iteration=300000, cost=431065.9299582231\n",
      "iteration=310000, cost=424295.40016185323\n",
      "iteration=320000, cost=440874.85483057675\n",
      "iteration=330000, cost=417277.3890537086\n",
      "iteration=340000, cost=411974.2565442774\n",
      "iteration=350000, cost=410730.59596816567\n",
      "iteration=360000, cost=411494.17042433517\n",
      "iteration=370000, cost=406922.2119615289\n",
      "iteration=380000, cost=408591.6949327063\n",
      "iteration=390000, cost=400107.8842351242\n",
      "iteration=400000, cost=414403.7927773239\n",
      "iteration=410000, cost=396503.9845491099\n",
      "iteration=420000, cost=394534.747889781\n",
      "iteration=430000, cost=392005.4235290594\n",
      "iteration=440000, cost=388648.74695334706\n",
      "iteration=450000, cost=388141.30795560416\n",
      "iteration=460000, cost=385195.28090662806\n",
      "iteration=470000, cost=388550.16011491226\n",
      "iteration=480000, cost=381609.3191789538\n",
      "iteration=490000, cost=379773.82065835\n",
      "training accuracy = 0.7218\n"
     ]
    }
   ],
   "source": [
    "def sgd_for_svm_demo(y, X):\n",
    "    \n",
    "    max_iter = 500000\n",
    "    gamma = 1\n",
    "    lambda_ = 0.00001\n",
    "    random.seed(2)\n",
    "    \n",
    "    num_examples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    # convert to list for main loop\n",
    "    w = list(w)\n",
    "    \n",
    "    def is_support(y_n, x_n, w):\n",
    "        \"\"\"a datapoint is support if max{} is not 0. \"\"\"\n",
    "        dot_prod = 0\n",
    "        for (x_n_i, w_i) in zip(x_n,w):\n",
    "            dot_prod += x_n_i*w_i\n",
    "        return y_n * dot_prod < 1\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        \n",
    "        # n = sample one data point uniformly at random data from x\n",
    "        n = random.randint(0,num_examples-1)\n",
    "        \n",
    "        x_n, y_n = X[n], y[n]\n",
    "        \n",
    "        # convert to list for main loop\n",
    "        x_n = list(x_n)\n",
    "        \n",
    "        ################ MAIN LOOP ##################\n",
    "        ############# GRADIENT UPDATE ###############\n",
    "        \n",
    "        if(is_support(y_n, x_n, w)):\n",
    "            grad = [- y_n * x_n_i + lambda_ * w_i for (x_n_i ,w_i) in zip(x_n,w)]\n",
    "            w = [w_i - gamma/(it+1) * grad_i for (w_i,grad_i) in zip(w,grad)]\n",
    "        \n",
    "        else:\n",
    "            grad = [lambda_ * w_i for w_i in w]\n",
    "            w = [w_i - gamma/(it+1) * grad_i for (w_i,grad_i) in zip(w,grad)]\n",
    "        ################ END OF LOOP ################\n",
    "        \n",
    "        if it % 10000 == 0:\n",
    "            cost = calculate_primal_objective(y, X, np.array(w), lambda_)\n",
    "            print(\"iteration={i}, cost={c}\".format(i=it, c=cost))\n",
    "    \n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, X, w)))\n",
    "\n",
    "sgd_for_svm_demo(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM\n",
    "#### Version du lab du cours de ML - inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_stochastic_gradient(y, X, w, lambda_, n):\n",
    "    \"\"\"compute the stochastic gradient of loss plus regularizer.\n",
    "    X: the dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    n: the index of the (one) datapoint we have sampled\n",
    "    \"\"\"\n",
    "    # Be careful about the constant N (size) term!\n",
    "    # The complete objective for SVM is a sum, not an average as in earlier SGD examples!\n",
    "    def is_support(y_n, x_n, w):\n",
    "        \"\"\"a datapoint is support if max{} is not 0. \"\"\"\n",
    "        return y_n * x_n @ w < 1\n",
    "    \n",
    "    x_n, y_n = X[n], y[n]\n",
    "    \n",
    "    grad = - y_n * x_n.T if is_support(y_n, x_n, w) else np.zeros_like(x_n.T)\n",
    "    grad = np.squeeze(grad) + lambda_ * w\n",
    "    \n",
    "    return grad"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
