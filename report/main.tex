\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\large\textbf{Systems for Data Science Project Report} \hfill  \textbf{Romain Choukroun (203917)}\\ 
\normalsize EPFL CS-449 \hfill \textbf{Augustin Prado (237289)}\\
Prof. C. Koch \hfill \textbf{Brune Bastide (223967)} \\
TA: Yu Kaicheng  \hfill Due Date: 21/05/2018

\section*{Distributed Synchronous and Asynchronous SGD
}
A single-threaded version of SGD is often not fast enough. There are many ways to implement a multi-threaded version of SGD. However, it is not very straightforward to implement an efficient parallel version of SGD due to its inherently sequential nature. We used the Hogwild paper \cite{Hogwild} to manage to obtain an efficient and fast solution to this problem.

\section*{Design and Implementation}
\subsection*{Machine Learning}
We have first labeled the dataset by looking whether each sample contains the word "CCAT" or not, assigning $-1$ and $+1$. The Support Vector Machine (SVM) algorithm tries to find the best hyperplane that separates our data points by maximising the margin --that is the distance between the hyperplane itself to hyperplane's closest data point.

We have implemented in Python a stochastic gradient descent for this binary classification problem. We have used the standard SVM loss function: Hinge loss function. Loss is computed using $\texttt{calculate\_loss}$ in $\texttt{svm\_function.py}$ file. Gradient is computed in function $\texttt{mini\_batch\_update}$ where you can send samples one by one to do the formal stochastic gradient descent.

\subsection*{Synchronous version: Systems decisions}

For this first milestone, we split the work among multiple workers. The whole data and weight vector (that uniquely determine a hyperplane) are both stored in the main server. This choice is justified since we are processing on one computer and every one has access to the same ram. These choices will obviously evolve as we move onto a cluster for the next milestone.

The server starts by waiting for a given number of client connections. As long as all clients are not connected, the server holds. When all clients have successfully connected, the server blocks further connections and at each iteration each worker receives a batch of data, alongside its corresponding labels and the current weight vector. Then, each worker node computes the gradient of the sample(s) it has just received using \texttt{mini\_batch\_update} and sends the result back to the server. The server waits for each worker's answer and then update the weight vector by summing all the gradients received. Once the weight vector is updated, the iteration can start again.

When the work is done, the clients de-authenticate themselves from the server and the server computes the accuracy on the testing set using the weights.

\subsection*{From a synchronous to an asynchronous version}
The previously proposed solution was correct and efficient in a synchronous environment. As the provided solution will operate in both synchronous and asynchronous environments, the architecture of the program has been modified: the server splits and sends a portion of the data to each worker at the beginning so as to avoid latency when workers have to access data. This was not a problem for the first milestone, as we were working on a single machine. 

\subsection*{Asynchronous version: construction, implementation, architecture and design}
\section*{Experiments}

% to comment sections out, use the command \ifx and \fi. Use this technique when writing your pre lab. For example, to comment something out I would do:
%  \ifx
%	\begin{itemize}
%		\item item1
%		\item item2
%	\end{itemize}	
%  \fi




\subsection*{Analysis \& Testing}


\section*{Final Evaluation}


\section*{Attachments and deliverables}
\texttt{run.sh}, 
\texttt{sgd\_svm\_server.py}, 
\texttt{sgd\_svm\_client.py}, 
\texttt{sgd\_svm.proto}, 
\texttt{data.py}, 
\texttt{sgd\_svm\_pb2.py}, 
\texttt{sgd\_svm\_pb2\_grpc.py}, 
\texttt{svm\_function.py}

\begin{thebibliography}{9}
\bibitem{Hogwild} Feng Niu, Benjamin Recht, Christopher Re and Stephen J. Wright\emph{Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}. New Jersey: Prentice Hall.
\bibitem{Stopping criteria} Robert Burbidge \emph{Stopping Criteria for SVMs} Statistics Section, Imperial College.
\end{thebibliography}

\end{document}
